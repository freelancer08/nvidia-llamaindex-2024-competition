{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install all required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Install required packages\n",
    "!pip install llama-index-embeddings-azure-openai\n",
    "!pip install llama-index-llms-azure-openai\n",
    "!pip install pyodbc\n",
    "!pip install matplotlib\n",
    "!pip install llama-index-callbacks-arize-phoenix\n",
    "!pip install --upgrade --quiet llama-index-embeddings-nvidia\n",
    "!pip install llama-index-core\n",
    "!pip install llama-index-readers-file\n",
    "!pip install llama-index-llms-nvidia\n",
    "!pip install llama-index-postprocessor-nvidia-rerank\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add observability and logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_index.core\n",
    "import os\n",
    "\n",
    "PHOENIX_API_KEY = \"<YOUR_API_KEY>\"\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\n",
    "llama_index.core.set_global_handler(\n",
    "    \"arize_phoenix\", endpoint=\"https://llamatrace.com/v1/traces\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")  # logging.DEBUG for more verbose output\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create NVIDIA endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create NVIDIA endpoints NIMS\n",
    "import os\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.nvidia import NVIDIA\n",
    "\n",
    "os.environ[\"NVIDIA_API_KEY\"] = \"<Your_API_KEY>\"\n",
    "llm = NVIDIA(model=\"meta/llama-3.1-70b-instruct\")\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"py-connectionString\"] = \"<Your_Connection_String>\"\n",
    "DATABASE_SYNC_URL = os.environ[\"py-connectionString\"]\n",
    "from llama_index.core import SQLDatabase\n",
    "from sqlalchemy import (\n",
    "    create_engine\n",
    ")\n",
    "engine = create_engine(DATABASE_SYNC_URL)\n",
    "sql_database = SQLDatabase(engine=engine\n",
    "    , schema=\"database\"\n",
    "    , view_support=True\n",
    "    , include_tables=[\n",
    "        \"kpi\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import text\n",
    "with engine.connect() as connection:\n",
    "    results = connection.execute(text(\"SELECT TOP 100 * FROM database.kpi\")).fetchall()\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import NLSQLRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# default retrieval (return_raw=True)\n",
    "nl_sql_retriever = NLSQLRetriever(\n",
    "    sql_database, tables=[\n",
    "        \"kpi\"\n",
    "                        ], return_raw=True\n",
    ")\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(nl_sql_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\n",
    "    \"Summarise the top sales per shop last week.\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\n",
    "    \"Provide the top 5 shops weekly sales for the past 3 years?\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "persist_dir = \"./.data\"\n",
    "if os.path.exists(persist_dir):\n",
    "    print(\"Loading index from storage\")\n",
    "    load_index_from_storage(\n",
    "        StorageContext.from_defaults(persist_dir=persist_dir)\n",
    "    )\n",
    "print(\"Creating index\")\n",
    "nodes = []\n",
    "dataset = {}\n",
    "with open(\"./dataset.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "for query, response in dataset.items():\n",
    "    node = TextNode(text=query)\n",
    "    node.metadata[\"response\"] = response\n",
    "    node.excluded_embed_metadata_keys.append(\"response\")\n",
    "    nodes.append(node)\n",
    "\n",
    "\n",
    "index = VectorStoreIndex(nodes)\n",
    "\n",
    "query_str=\"Provide the top 5 shops weekly sales for the past 3 years?\"\n",
    "retriever = index.as_retriever(\n",
    "    top_k=2\n",
    ")\n",
    "nodes = retriever.retrieve(query_str)\n",
    "filtered_nodes = list(filter(lambda node: node.score > 0.5, nodes))\n",
    "# print(filtered_nodes.__len__())\n",
    "few_shot_examples = []\n",
    "for node in filtered_nodes:\n",
    "    query = node.text\n",
    "    response = node.metadata[\"response\"]\n",
    "    few_shot_examples.append(f\"Query: {query}\\nResponse: {response}\")\n",
    "    # print(\"Check this line\")\n",
    "\n",
    "to_return = (\n",
    "    (\n",
    "        f\"Below are some examples of the structure of your response:\\n\"\n",
    "        + \"\\n---\\n\".join(few_shot_examples)\n",
    "    )\n",
    "    if few_shot_examples\n",
    "    else \"\"\n",
    ")\n",
    "\n",
    "nl_sql_retriever = NLSQLRetriever(\n",
    "    sql_database, tables=[\n",
    "        \"kpi\"\n",
    "                        ]\n",
    "    , return_raw=True\n",
    "    , context_str_prefix=to_return\n",
    ")\n",
    "query_engine = RetrieverQueryEngine.from_args(nl_sql_retriever)\n",
    "response = query_engine.query(query_str)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Test UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def generate_text(prompt):\n",
    "    # generated_text = agent.chat(prompt)\n",
    "    generated_text = query_engine.query(prompt)\n",
    "    return generated_text\n",
    "\n",
    "iface = gr.Interface(fn=generate_text, inputs=\"text\", outputs=\"text\").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
